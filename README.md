# Projet Scrapy Books avec API

Ce projet combine un scraper Scrapy pour [books.toscrape.com](http://books.toscrape.com/) et une API RESTful FastAPI pour acc√©der aux donn√©es collect√©es.

## üìã Fonctionnalit√©s

- **Scraping** : Extraction des donn√©es de livres depuis books.toscrape.com
- **Stockage** : Sauvegarde dans une base de donn√©es SQLite
- **API REST** : Interface pour acc√©der aux donn√©es via des endpoints HTTP
- **Recherche** : Recherche de livres par titre, cat√©gorie, prix, etc.
- **Statistiques** : M√©triques sur la collection de livres

## üöÄ Installation

1. **Cloner le d√©p√¥t**
   ```bash
   git clone [URL_DU_REPO]
   cd Scrapy_Books
   ```

2. **Cr√©er un environnement virtuel**
   ```bash
   python -m venv env
   env\Scripts\activate  # Sur Windows
   source env/bin/activate  # Sur macOS/Linux
   ```

3. **Installer les d√©pendances**
   ```bash
   pip install -r requirements.txt
   ```

## üï∑Ô∏è Utilisation du Scraper

1. **Lancer le scraper**
   ```bash
   cd monprojet
   scrapy crawl books
   ```
   
   Les donn√©es seront sauvegard√©es dans `monprojet/books.db`

## üåê Utilisation de l'API

1. **D√©marrer le serveur API**
   ```bash
   cd books_api
   uvicorn app.main:app --reload --port 8000
   ```

2. **Acc√©der √† la documentation**
   - Documentation interactive: http://localhost:8000/docs
   - Documentation alternative: http://localhost:8000/redoc

## üìÇ Structure du projet

```
Scrapy_Books/
‚îÇ
‚îú‚îÄ‚îÄ monprojet/                  # Projet Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ spiders/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scrapybooks.py      # Spider principal
‚îÇ   ‚îú‚îÄ‚îÄ items.py                # D√©finition des items
‚îÇ   ‚îú‚îÄ‚îÄ itemloaders.py          # Loaders et nettoyages
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py            # Pipelines de traitement
‚îÇ   ‚îú‚îÄ‚îÄ settings.py             # Configuration Scrapy
‚îÇ   ‚îî‚îÄ‚îÄ books.db                # Base de donn√©es SQLite (g√©n√©r√©e)
‚îÇ
‚îú‚îÄ‚îÄ books_api/                  # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/              # Configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/              # Acc√®s aux donn√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/            # Logique m√©tier
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ presentation/      # Routes et sch√©mas
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt        # D√©pendances de l'API
‚îÇ
‚îú‚îÄ‚îÄ requirements-unified.txt    # Toutes les d√©pendances
‚îî‚îÄ‚îÄ README.md                  # Ce fichier
```

## üìö Endpoints de l'API

- `GET /books/` - Liste tous les livres
- `GET /books/{book_id}` - D√©tails d'un livre sp√©cifique
- `GET /books/search/title/{title}` - Recherche de livres par titre
- `GET /books/search/category/{category}` - Recherche par cat√©gorie
- `GET /books/stats/summary` - Statistiques sur la collection

## Configuration

Cr√©ez un fichier `.env` √† la racine du projet avec les variables n√©cessaires :

```env
# Configuration de la base de donn√©es
DB_TYPE=sqlite
SQLITE_PATH=./monprojet/books.db

# Pour PostgreSQL (optionnel)
# PG_DB=books_db
# PG_USER=postgres
# PG_PASSWORD=votre_mot_de_passe
# PG_HOST=localhost
# PG_PORT=5432
```

## Ex√©cution

1. **Lancer le scraper**
   ```bash
   cd monprojet
   scrapy crawl books
   ```

2. **D√©marrer l'API**
   ```bash
   cd books_api
   uvicorn app.main:app --reload --port 8000
   ```

## V√©rification des donn√©es

### SQLite
```bash
sqlite3 monprojet/books.db
sqlite> SELECT COUNT(*) FROM books;
sqlite> SELECT * FROM books LIMIT 5;
```

### Via l'API
- Acc√©dez √† la documentation interactive : http://localhost:8000/docs
- Ou consultez directement : http://localhost:8000/books/

## Notes techniques

- Les doublons sont automatiquement ignor√©s (bas√© sur l'URL)
- Le middleware g√®re automatiquement les User-Agents
- La pagination est g√©r√©e automatiquement par le spider