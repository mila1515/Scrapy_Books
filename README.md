# üìö Projet Scrapy Books avec API REST

Ce projet combine un scraper Scrapy pour [books.toscrape.com](http://books.toscrape.com/) et une API RESTful FastAPI simplifi√©e pour acc√©der aux donn√©es collect√©es.

## üöÄ Fonctionnalit√©s

### Scraping
- Extraction compl√®te des donn√©es de livres depuis books.toscrape.com
- Gestion des cat√©gories et des d√©tails des livres
- Sauvegarde dans une base de donn√©es SQLite (`monprojet/books.db`)

### API REST
- Authentification HTTP Basic simple
- Endpoints RESTful pour acc√©der aux livres et cat√©gories
- Documentation interactive (Swagger/ReDoc)
- Filtrage par cat√©gorie et prix
- Configuration CORS pour le d√©veloppement
- Gestion des erreurs et reprise sur erreur

## üõ† Installation

### Pr√©requis
- Python 3.8+
- pip (gestionnaire de paquets Python)
- Git (optionnel)

### Configuration

1. **Cloner le d√©p√¥t**
   ```bash
   git clone [URL_DU_REPO]
   cd Scrapy_Books
   ```

2. **Cr√©er et activer un environnement virtuel** (recommand√©)
   ```bash
   # Sur Windows
   python -m venv venv
   .\venv\Scripts\activate
   
   # Sur macOS/Linux
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Installer les d√©pendances**
   ```bash
   # Installer les d√©pendances du scraping
   pip install -r requirements.txt
   
   # Installer les d√©pendances de l'API
   cd books_api
   pip install -r requirements.txt
   cd ..
   ```

## üöÄ Utilisation

### 1. Lancer le scraper

```bash
cd monprojet
scrapy crawl books -o books.json
```

### 2. D√©marrer l'API

```bash
cd books_api
uvicorn app.main:app --reload
```

L'API sera disponible √† l'adresse : http://127.0.0.1:8000

## üìö Documentation de l'API

### Authentification
L'API utilise l'authentification HTTP Basic :
- **Utilisateur** : `admin`
- **Mot de passe** : `admin123`

### Endpoints disponibles

#### Livres
- `GET /api/books/` - Liste tous les livres
  - Param√®tres optionnels :
    - `category` : Filtrer par cat√©gorie
    - `min_price` : Prix minimum
    - `max_price` : Prix maximum

- `GET /api/books/{book_id}` - R√©cup√®re un livre par son ID

- `GET /api/books/categories/` - Liste toutes les cat√©gories disponibles

### Documentation interactive
- **Swagger UI** : http://127.0.0.1:8000/docs
- **ReDoc** : http://127.0.0.1:8000/redoc

## üîß Configuration avanc√©e

### Changer les identifiants d'acc√®s
Pour modifier les identifiants d'acc√®s, √©ditez le fichier :
`books_api/app/presentation/auth_routes.py`

### Configuration CORS
Pour modifier les param√®tres CORS, √©ditez le fichier :
`books_api/app/main.py`

## üìù Exemples de requ√™tes

### Avec cURL
```bash
# R√©cup√©rer tous les livres
curl -u admin:admin123 http://localhost:8000/api/books/

# Filtrer par cat√©gorie
curl -u admin:admin123 "http://localhost:8000/api/books/?category=Fiction"

# R√©cup√©rer un livre sp√©cifique
curl -u admin:admin123 http://localhost:8000/api/books/1
```

### Avec Python (requests)
```python
import requests
from requests.auth import HTTPBasicAuth

# Configuration de base
BASE_URL = "http://localhost:8000/api"
auth = HTTPBasicAuth("admin", "admin123")

# R√©cup√©rer tous les livres
response = requests.get(f"{BASE_URL}/books/", auth=auth)
print(response.json())

# Filtrer par cat√©gorie
response = requests.get(
    f"{BASE_URL}/books/",
    params={"category": "Fiction"},
    auth=auth
)
print(response.json())
```

## ü§ù Contribution

Les contributions sont les bienvenues ! Voici comment contribuer :

1. Forkez le projet
2. Cr√©ez une branche pour votre fonctionnalit√© (`git checkout -b feature/ma-fonctionnalite`)
3. Committez vos changements (`git commit -am 'Ajouter ma fonctionnalit√©'`)
4. Poussez vers la branche (`git push origin feature/ma-fonctionnalite`)
5. Cr√©ez une Pull Request

## üìù Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## üìû Contact

Pour toute question ou suggestion, veuillez ouvrir une issue sur le d√©p√¥t.

## üìã Pr√©requis

- Python 3.8 ou sup√©rieur
- pip (gestionnaire de paquets Python)
- SQLite (inclus avec Python)

## üõ† Installation

1. **Cloner le d√©p√¥t**
   ```bash
   git clone https://github.com/votre-utilisateur/Scrapy_Books.git
   cd Scrapy_Books
   ```

2. **Cr√©er et activer un environnement virtuel**
   ```bash
   # Windows
   python -m venv venv
   .\venv\Scripts\activate

   # macOS/Linux
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Installer les d√©pendances**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurer les variables d'environnement**
   Cr√©ez un fichier `.env` √† la racine du projet :
   ```env
   # Configuration de la base de donn√©es
   DATABASE_URL=sqlite:///monprojet/books.db
   
   # Configuration JWT
   SECRET_KEY=votre_cl√©_secr√®te_tr√®s_longue_et_s√©curis√©e
   ALGORITHM=HS256
   ACCESS_TOKEN_EXPIRE_MINUTES=30
   ```

## üï∑ Utilisation du Scraper

1. **Lancer le scraper**
   ```bash
   cd monprojet
   scrapy crawl books
   ```
   
   Les donn√©es seront sauvegard√©es dans `monprojet/books.db`

2. **Options de scraping**
   ```bash
   # Limiter le nombre de pages √† scraper
   scrapy crawl books -s CLOSESPIDER_PAGECOUNT=10
   
   # Changer le niveau de log
   scrapy crawl books --loglevel=INFO
   
   # Sauvegarder les r√©sultats dans un fichier JSON
   scrapy crawl books -o livres.json
   ```

## üåê Utilisation de l'API

1. **D√©marrer le serveur API**
   ```bash
   # Mode d√©veloppement (avec rechargement automatique)
   uvicorn books_api.app.main:app --reload --port 8000
   
   # Mode production
   uvicorn books_api.app.main:app --host 0.0.0.0 --port 8000 --workers 4
   ```

2. **Acc√©der √† la documentation**
   - Swagger UI (interactive): http://localhost:8000/docs
   - ReDoc (documentation): http://localhost:8000/redoc

3. **Authentification**
   ```bash
   # Obtenir un token
   curl -X POST "http://localhost:8000/api/v1/token" \
   -H "Content-Type: application/x-www-form-urlencoded" \
   -d "username=admin&password=admin123"
   
   # Utiliser le token
   curl -X GET "http://localhost:8000/api/v1/books/" \
   -H "Authorization: Bearer VOTRE_TOKEN_JWT"
   ```

## üèó Structure du projet

```
Scrapy_Books/
‚îÇ
‚îú‚îÄ‚îÄ monprojet/                  # Projet Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ spiders/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scrapybooks.py      # Spider principal
‚îÇ   ‚îú‚îÄ‚îÄ items.py                # D√©finition des items
‚îÇ   ‚îú‚îÄ‚îÄ itemloaders.py          # Loaders et nettoyages
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py            # Pipelines de traitement
‚îÇ   ‚îî‚îÄ‚îÄ settings.py             # Configuration Scrapy
‚îÇ
‚îú‚îÄ‚îÄ books_api/                  # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/              # Acc√®s aux donn√©es
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py      # Configuration et s√©curit√©
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py      # Mod√®les SQLAlchemy
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repositories/   # R√©pertoires d'acc√®s aux donn√©es
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/            # Logique m√©tier
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/      # Entit√©s du domaine
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/      # Services m√©tier
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ presentation/      # Couche pr√©sentation
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ api/           # Routeurs FastAPI
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ schemas.py     # Mod√®les Pydantic
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ dependencies.py # D√©pendances d'API
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tests/                 # Tests automatis√©s
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_*.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ main.py                # Point d'entr√©e de l'application
‚îÇ
‚îú‚îÄ‚îÄ .env.example               # Exemple de configuration
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                  # Ce fichier

```

## üîß Ex√©cution des tests

```bash
# Ex√©cuter tous les tests
pytest books_api/tests/ -v

# Ex√©cuter les tests avec couverture de code
pytest --cov=books_api books_api/tests/

# G√©n√©rer un rapport de couverture HTML
pytest --cov=books_api --cov-report=html books_api/tests/
```

## üìä V√©rification des donn√©es

### SQLite
```bash
sqlite3 monprojet/books.db
sqlite> SELECT COUNT(*) FROM books;
sqlite> SELECT * FROM books LIMIT 5;
```

### Via l'API
- Documentation interactive : http://localhost:8000/docs
- Documentation alternative : http://localhost:8000/redoc
- Endpoint des livres : http://localhost:8000/api/v1/books/

## üîí S√©curit√©

- Authentification JWT requise pour les op√©rations sensibles
- Hachage des mots de passe avec bcrypt
- Protection contre les attaques CSRF
- Headers de s√©curit√© HTTP
- Validation stricte des entr√©es

## üöÄ D√©ploiement

### Pr√©paration pour la production
1. Mettre √† jour les variables d'environnement dans `.env`
2. Changer la cl√© secr√®te JWT
3. Configurer une base de donn√©es de production (PostgreSQL recommand√©)
4. Configurer un serveur web (Nginx/Apache) avec Gunicorn/Uvicorn

### Variables d'environnement requises
```env
# Production
SECRET_KEY=votre_cl√©_secr√®te_tr√®s_longue_et_s√©curis√©e
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
DATABASE_URL=postgresql://user:password@localhost:5432/books_db

# D√©veloppement
# DATABASE_URL=sqlite:///monprojet/books.db
```

## ü§ù Contribution

1. Fork le projet
2. Cr√©ez une branche (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Poussez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## üìù Licence

Distribu√© sous la licence MIT. Voir `LICENSE` pour plus d'informations.

## üìû Contact

Votre Nom - [@votre_twitter](https://twitter.com/votre_twitter) - email@exemple.com

Lien du projet : [https://github.com/votre-utilisateur/Scrapy_Books](https://github.com/votre-utilisateur/Scrapy_Books)

## üôè Remerciements

- [FastAPI](https://fastapi.tiangolo.com/)
- [Scrapy](https://scrapy.org/)
- [SQLAlchemy](https://www.sqlalchemy.org/)
- [Pydantic](https://pydantic-docs.helpmanual.io/)
- [Uvicorn](https://www.uvicorn.org/)

- Les doublons sont automatiquement ignor√©s (bas√© sur l'URL)
- Le middleware g√®re automatiquement les User-Agents
- La pagination est g√©r√©e automatiquement par le spider