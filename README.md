# Scrapy Books - Outil de Scraping et API REST

Un projet complet pour extraire des donnÃ©es de livres depuis le web et les exposer via une API RESTful sÃ©curisÃ©e.

## ğŸ“‹ FonctionnalitÃ©s

### ğŸ•·ï¸ Module de Scraping
- Extraction de donnÃ©es de livres depuis des sites web
- Gestion des requÃªtes asynchrones
- Nettoyage et traitement des donnÃ©es
- Stockage dans une base de donnÃ©es SQLite

### ğŸŒ API REST
- Documentation interactive (Swagger/ReDoc)
- Authentification sÃ©curisÃ©e
- Gestion complÃ¨te des livres (CRUD)
- Recherche avancÃ©e par catÃ©gories, titres, etc.
- Statistiques sur la collection de livres

## ğŸš€ PrÃ©requis

- Python 3.8+
- pip (gestionnaire de paquets Python)
- Git (optionnel)

## ğŸ›  Installation

1. **Cloner le dÃ©pÃ´t**
   ```bash
   git clone [URL_DU_REPO]
   cd Scrapy_Books
   ```

2. **CrÃ©er un environnement virtuel**
   ```bash
   python -m venv venv
   ```

3. **Activer l'environnement virtuel**
   - Windows : `.\venv\Scripts\activate`
   - macOS/Linux : `source venv/bin/activate`

4. **Installer les dÃ©pendances**
   ```bash
   pip install -r requirements.txt
   ```

## âš™ Configuration

1. **Configurer l'environnement**
   CrÃ©ez un fichier `.env` dans le dossier `monprojet/` :
   ```env
   # Authentification API
   ADMIN_USERNAME=admin
   ADMIN_PASSWORD=votre_mot_de_passe_secure
   
   # Base de donnÃ©es
   DATABASE_URL=sqlite:///monprojet/books.db
   
   # Configuration Scrapy (si nÃ©cessaire)
   USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) ..."
   ```

## ğŸ•·ï¸ Utilisation du Scraper

1. **Lancer le spider**
   ```bash
   cd monprojet
   scrapy crawl nom_du_spider -o livres.json
   ```

   Options disponibles :
   - `-o output.json` : Exporter les rÃ©sultats
   - `-s LOG_LEVEL=INFO` : Niveau de logs
   - `-a param=valeur` : Passer des paramÃ¨tres au spider

2. **ExÃ©cuter les tests du scraper**
   ```bash
   cd monprojet
   scrapy check
   ```

## ğŸŒ Utilisation de l'API

1. **DÃ©marrer le serveur**
   ```bash
   python run.py
   ```

2. **AccÃ¨s Ã  la documentation**
   - Swagger UI : http://127.0.0.1:8000/docs
   - ReDoc : http://127.0.0.1:8000/redoc

3. **Authentification**
   ```
   Utilisateur: admin
   Mot de passe: [votre_mot_de_passe]
   ```

## ğŸ§ª Tests

Le projet inclut une suite complÃ¨te de tests pour assurer la qualitÃ© et la fiabilitÃ© du code. Voici comment exÃ©cuter et gÃ©rer les tests :

### ExÃ©cuter tous les tests

```bash
# Depuis la racine du projet
python -m pytest books_api/tests/ -v
```



## ğŸ“š Structure du Projet

```
Scrapy_Books/
â”œâ”€â”€ books_api/                  # Code source de l'API
â”‚   â”œâ”€â”€ data/                   # AccÃ¨s aux donnÃ©es
â”‚   â”‚   â”œâ”€â”€ book_repository.py  # Gestion des opÃ©rations CRUD
â”‚   â”‚   â”œâ”€â”€ config.py           # Configuration de l'application
â”‚   â”‚   â””â”€â”€ sqlite.py           # Gestion de la base de donnÃ©es
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                 # Logique mÃ©tier
â”‚   â”‚   â”œâ”€â”€ book.py            # ModÃ¨le de donnÃ©es Livre
â”‚   â”‚   â”œâ”€â”€ book_usecases.py   # Cas d'utilisation mÃ©tier
â”‚   â”‚   â””â”€â”€ user.py            # Gestion des utilisateurs
â”‚   â”‚
â”‚   â””â”€â”€ presentation/          # Gestion des requÃªtes
â”‚       â”œâ”€â”€ auth.py            # Authentification
â”‚       â”œâ”€â”€ main.py            # Application FastAPI
â”‚       â””â”€â”€ routes.py          # DÃ©finition des routes API
â”‚
â”œâ”€â”€ monprojet/                 # Projet Scrapy
â”‚   â””â”€â”€ monprojet/
â”‚       â”œâ”€â”€ spiders/           # Spiders de scraping
â”‚       â”‚   â”œâ”€â”€ __init__.py   # Fichier d'initialisation
â”‚       â”‚   â””â”€â”€ scrapybooks.py  # Spider principal pour le scraping de livres
â”‚       â”‚
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ items.py          # ModÃ¨les de donnÃ©es Scrapy
â”‚       â”œâ”€â”€ middlewares.py    # Middlewares personnalisÃ©s
â”‚       â”œâ”€â”€ pipelines.py      # Traitement des donnÃ©es
â”‚       â”œâ”€â”€ settings.py       # Configuration Scrapy
â”‚       â””â”€â”€ .env             # Configuration d'environnement
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt         # DÃ©pendances du projet
â””â”€â”€ run.py                   # Point d'entrÃ©e de l'application
```

## ğŸ§ª Tests

### Tester l'API
```bash
pytest books_api/tests/
```

### Tester les spiders
```bash
cd monprojet
scrapy check
```
