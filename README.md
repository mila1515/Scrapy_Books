# Scrapy Books - Scraper avec SQLite et PostgreSQL

Ce projet Scrapy scrape le site [books.toscrape.com](http://books.toscrape.com/) et stocke les donnÃ©es des livres dans **SQLite** et **PostgreSQL** en parallÃ¨le.

---

## ğŸ“‚ Structure du projet

monprojet/
â”‚
â”œâ”€â”€ spiders/
â”‚ â””â”€â”€ scrapybooks.py # Spider principal
â”‚
â”œâ”€â”€ items.py # DÃ©finition des items
â”œâ”€â”€ itemloaders.py # Loaders et nettoyages
â”œâ”€â”€ pipelines.py # Pipelines SQLite et PostgreSQL
â”œâ”€â”€ middlewares.py # Middleware avec User-Agent
â”œâ”€â”€ settings.py # ParamÃ¨tres Scrapy
â”œâ”€â”€ books.db # Base SQLite (aprÃ¨s le premier run)
â”œâ”€â”€ .env # Variables d'environnement
â””â”€â”€ scrapy.cfg


## âš™ï¸ Installation

1. Clone le projet :

```bash
git clone <repo_url>
cd monprojet


2. CrÃ©e et active un environnement virtuel (optionnel mais recommandÃ©) :
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows


3. Installe les dÃ©pendances :

pip install -r requirements.txt


4.  âš™ï¸ Configuration du fichier `.env`

CrÃ©e un fichier `.env` Ã  la racine du projet et ajoute tes informations de base de donnÃ©es comme suitÂ :

```env
# Chemin vers la base SQLite
SQLITE_PATH=  chemin complet vers ta base SQLite.
# Informations PostgreSQL
PG_DB= nom de la base PostgreSQL.
PG_USER= utilisateur PostgreSQL.
PG_PASSWORD= mot de passe PostgreSQL.
PG_HOST= hÃ´te PostgreSQL (souvent localhost).
PG_PORT= port PostgreSQL (par dÃ©faut 5432).




## ğŸƒ Lancer le spider
scrapy crawl scrapybooks

Les donnÃ©es seront ajoutÃ©es dans SQLite (books.db) et dans PostgreSQL (books_db).
Les doublons sur lâ€™URL sont ignorÃ©s.
Si le stock est introuvable, la valeur est -1 et un warning est affichÃ© dans les logs.


## ğŸ”§ VÃ©rification des bases

SQLite:

sqlite3 books.db
sqlite> SELECT COUNT(*) FROM books;
sqlite> SELECT * FROM books LIMIT 5;


PostgreSQL:

-- Connecte-toi Ã  la base
\c books_db
SELECT COUNT(*) FROM books;
SELECT * FROM books LIMIT 5;


âš¡ Notes

User-Agent gÃ©rÃ© via middleware pour rotation ou randomisation.
Stock : si absent, devient -1 dans les bases.
Pagination automatique.
Pipelines SQLite et PostgreSQL en parallÃ¨le.